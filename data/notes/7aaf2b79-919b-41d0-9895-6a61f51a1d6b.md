
MapReduce is a programming model for processing large amounts of data in bulk across many machines

The map and reduce functions are somewhat restricted in what they are allowed to do. They must be pure functions, which means they only use the data that is passed to them as input, they cannot perform additional database queries, and they must not have any side effects.

- **what**: do parellel in map phase, do merge sort in sort, do linear in reducer
- **why**: quick in map, slow in reduce
- **how**: could do select, filter, min,max,sum, no avg, join is not a good operation in mapreduce, only in reduce phase.
- sort:
    - search in the final result
    - mapper yield duplicated key and we need use sort to group them in one reducer 
- **problem**
    - A single **master** process called **JobTracker**, which **coordinates** all jobs running on the cluster and **assigns** map and reduce tasks to run on the TaskTrackers
    - A number of subordinate processes called **TaskTrackers**, which run assigned tasks and periodically report the progress to the JobTracker
    - Hadoop was designed to run MapReduce jobs only
    - more maps means more overhead for master to keep track of the state of computation
    - fewer map-reduce phase
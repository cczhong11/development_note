<h1 id="io"><a aria-hidden="true" class="anchor-heading icon-link" href="#io"></a>Io</h1>
<p>□　I/O等待。 CPU等待一个I/O操作的发生所花费时间。较高和持续的值很多时候可能表明存在一个I/O瓶颈。</p>
<p>□　平均队列长度。 未完成的I/O请求数量。一般情况下，一个磁盘有2到3个队列是最佳的；较高的值可能表明有一个磁盘I/O瓶颈。</p>
<p>□　平均等待时间。 服务一个I/O请求所测量的平均时间，以毫秒为单位。等待时间是由实际的I/O操作和它在I/O队列中等待的时间组成的。</p>
<p>□　每秒传输。 描述了每秒钟多少个I/O操作被执行（读和写）。该指标要结合每秒kB值，以帮助你确定系统的平均传输大小。平均传输大小一般应该与你的磁盘子系统使用的条带大小相匹配。</p>
<h2 id="6-iostat--xz-1"><a aria-hidden="true" class="anchor-heading icon-link" href="#6-iostat--xz-1"></a>6. iostat -xz 1</h2>
<pre><code>$ iostat -xz 1
Linux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015  _x86_64_ (32 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          73.96    0.00    3.73    0.03    0.06   22.21

Device:   rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
xvda        0.00     0.23    0.21    0.18     4.52     2.08    34.37     0.00    9.98   13.80    5.42   2.44   0.09
xvdb        0.01     0.00    1.02    8.94   127.97   598.53   145.79     0.00    0.43    1.78    0.28   0.25   0.25
xvdc        0.01     0.00    1.02    8.86   127.79   595.94   146.50     0.00    0.45    1.82    0.30   0.27   0.26
dm-0        0.00     0.00    0.69    2.32    10.47    31.69    28.01     0.01    3.23    0.71    3.98   0.13   0.04
dm-1        0.00     0.00    0.00    0.94     0.01     3.78     8.00     0.33  345.84    0.04  346.81   0.01   0.00
dm-2        0.00     0.00    0.09    0.07     1.35     0.36    22.50     0.00    2.55    0.23    5.62   1.78   0.03
[...]
</code></pre>
<p>This is a great tool for understanding <strong>block devices</strong> (disks), both the workload applied and the resulting performance. Look for:
r/s, w/s, rkB/s, wkB/s: These are the delivered reads, writes, read Kbytes, and write Kbytes per second to the device. Use these for workload characterization. <strong>A performance problem may simply be due to an excessive load applied.</strong>
await: The average time for the I/O in milliseconds. This is the time that the application suffers, as it includes both time queued and time being serviced. <strong>Larger than expected average times can be an indicator of device saturation, or device problems.</strong>
avgqu-sz: The average number of requests issued to the device. Values greater than 1 can be evidence of saturation (although devices can typically operate on requests in parallel, especially virtual devices which front multiple back-end disks.)
%util: Device utilization. This is really a busy percent, showing the time each second that the device was doing work. Values greater than 60% typically lead to poor performance (which should be seen in await), although it depends on the device. Values close to 100% usually indicate saturation.
If the storage device is a logical disk device fronting many back-end disks, then 100% utilization may just mean that some I/O is being processed 100% of the time, however, the back-end disks may be far from saturated, and may be able to handle much more work.
Bear in mind that poor performing disk I/O isn’t necessarily an application issue. Many techniques are typically used to perform I/O asynchronously, so that the application doesn’t block and suffer the latency directly (e.g., read-ahead for reads, and buffering for writes).</p>
<p>iostat报告CPU统计数据，并观察有关设备的平均传输速率的活跃时间，其用来监控系统输入／输出设备负载。iostat命令生成的报告可以用来更改系统配置，进行详细的I/O瓶颈和性能调整，以在物理磁盘之间更好地平衡输入/输出负载。参考4.4.1节的内容。iostat工具是sysstat软件包的一部分。</p>
<p>通过iostat命令生成的第一部分报告是CPU使用率报告。对于多处理器系统，它是所有处理器总数的CPU全局平均值。报告包含以下格式：</p>
<p>□　%user，显示当在用户级别（应用程序）执行时发生的CPU使用率百分比。</p>
<p>□　%nice，显示在拥有nice优先级用户级别执行时发生的CPU使用率百分比。</p>
<p>□　%system，显示当在系统级别（内核）执行时发生的CPU使用率百分比。</p>
<p>□　%iowait，显示在系统未完成磁盘I/O请求期间，一个CPU或多个CPU空闲时间的百分比。</p>
<p>□　%steal，显示当hypervisor为另一个虚拟处理器服务的时候，一个虚拟CPU或多个虚拟CPU非自愿等待所花费时间的百分比。</p>
<p>□　%idle，显示一个CPU或多个CPU空闲时间的百分比，并且系统没有未完成的磁盘I/O请求。</p>
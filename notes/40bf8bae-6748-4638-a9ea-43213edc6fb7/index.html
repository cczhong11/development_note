<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><title>Zippydb</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal knowledge space"/><meta property="og:title" content="Zippydb"/><meta property="og:description" content="Personal knowledge space"/><meta property="og:url" content="https://notes.tczhong.com/notes/40bf8bae-6748-4638-a9ea-43213edc6fb7/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="4/6/2021"/><meta property="article:modified_time" content="4/6/2021"/><link rel="canonical" href="https://notes.tczhong.com/notes/40bf8bae-6748-4638-a9ea-43213edc6fb7/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/_next/static/css/55969ab92b38e37f.css" as="style"/><link rel="stylesheet" href="/_next/static/css/55969ab92b38e37f.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-8e0b6832cc385016.js" defer=""></script><script src="/_next/static/chunks/framework-bb5c596eafb42b22.js" defer=""></script><script src="/_next/static/chunks/main-c4b0e551a2150d17.js" defer=""></script><script src="/_next/static/chunks/pages/_app-274947796022cf6e.js" defer=""></script><script src="/_next/static/chunks/155-3a6c02d7e042edb7.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bid%5D-dc9b1cbe134a7143.js" defer=""></script><script src="/_next/static/BJJu79SH0O6jPLyBmlOdc/_buildManifest.js" defer=""></script><script src="/_next/static/BJJu79SH0O6jPLyBmlOdc/_ssgManifest.js" defer=""></script><script src="/_next/static/BJJu79SH0O6jPLyBmlOdc/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout site-layout" style="margin-top:64px"><section class="ant-layout site-layout" style="flex-direction:row"><section class="ant-layout site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></section><section class="ant-layout side-layout-main" style="max-width:1200px;display:initial"><main class="ant-layout-content main-content" role="main" style="padding:0 24px"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="zippydb"><a aria-hidden="true" class="anchor-heading" href="#zippydb"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Zippydb</h1>
<h1 id="zippydb-1"><a aria-hidden="true" class="anchor-heading" href="#zippydb-1"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>zippydb</h1>
<p>fb internal distributed key-value databse</p>
<p>single node based on rockdb</p>
<h2 id="arch"><a aria-hidden="true" class="anchor-heading" href="#arch"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>arch</h2>
<p>ZippyDB servers can be mapped to the roles in the Multi-Paxos protocol as follows:</p>
<p>Primary server = proposer/leader + acceptor + learner
Secondary server = acceptor + learner
Follower server = learner (An ordinary ZippyDB user may safely skip the details of the mapping above, as it is only intended to map terminology for readers who are familiar with Paxos.)</p>
<h2 id="write"><a aria-hidden="true" class="anchor-heading" href="#write"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>write</h2>
<p><img src="/assets/images/2021-04-05-17-12-35.png"></p>
<h2 id="read"><a aria-hidden="true" class="anchor-heading" href="#read"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>read</h2>
<p><img src="/assets/images/2021-04-05-17-13-08.png"></p>
<h2 id="primary-failover"><a aria-hidden="true" class="anchor-heading" href="#primary-failover"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>primary failover</h2>
<p>When the primary fails, Zeus detects the failure through lost heartbeats and notifies ShardManager. ShardManager chooses a most appropriate secondary and sends it a message to convert it into a new primary. </p>
<p>Specifically in this example, it is possible that, right before server X crashed, server X and server Y worked together to accept a write and server X has already sent the "write-success" response back to the client, but that write has not yet reached server Z. This is because server X and server Y form a majority and hence can accept a write without waiting for the "accepted" confirmation from server Z. Actually, server Z may never get the "accept" notification for this write from server X before server X crashed, e.g., due to slow network between server X and server Z. In this case, after server Z becomes the new primary, it needs to contact server Y to recover the missing write. The prepare phase of the Paxos protocol guarantees that the new primary will correctly discover all those missing writes, so long as only a minority of the replicas have failed.
After bringing its local replica up-to-date, the new primary (server Z in this example) starts to handle reads and writes as normal. As shown in the figure below, the write path executes the following steps in sequence:</p>
<ol>
<li>When the client wants to send a new "Put()" request, the ServiceRouter library linked into the client notices that ShardManager has changed the shard's primary assignment, and automatically routes the request to the new primary (server Z).</li>
<li>Server Z asks both server X and server Y to "accept" the write.</li>
<li>Server Z gets the "accepted" confirmation from server Y, but gets no confirmation from server X.</li>
<li>Since server Z collects a majority votes for the write (one from itself and another from server Y), server Z decides to commit the write and sends a "write-success" response back to the client. Server Z does not wait for server X to recover in order to accept the write. In other words, once server Z becomes the new primary, the failure of server X does not affect the database's availability .</li>
<li>Server Z sends a "commit" message to server Y to inform server Y that an agreement has been reached on the write. This step is not shown in the figure for brevity.</li>
</ol>
<p><img src="/assets/images/2021-04-05-17-15-33.png"></p>
<p>The process of primary fail-over takes about 10 seconds, including the time for Zeus to detect lost heartbeats from the old primary</p>
<h2 id="a-failed-replica-rejoins"><a aria-hidden="true" class="anchor-heading" href="#a-failed-replica-rejoins"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>A Failed Replica Rejoins</h2>
<p>Server X synchronizes with the new primary (server Z) to bring its local database up-to-date. It then starts to process protocol messages as normal. See the example in the figure below.</p>
<p><img src="/assets/images/2021-04-05-17-16-52.png"></p>
<h2 id="sharding-and-load-balancing"><a aria-hidden="true" class="anchor-heading" href="#sharding-and-load-balancing"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Sharding and Load Balancing</h2>
<p>In practice, each server runs a single ZippyDB process that can host multiple shards. In the figure below, each server hosts three shards: a primary for one shard, and two secondaries for two other shards. For example, server 1 hosts shard A's primary, shard D's secondary, and shard E's secondary. The three replicas of shard A are distributed across server 1 (primary), server 2 (secondary), and server 5 (secondary). ShardManager considers multiple factors in shard placement. In this figure, ShardManager places exactly one primary on each server for the purpose of load balancing, because a primary incurs a higher load than a secondary does. ShardManager may also place the different replicas of a shard across different clusters for the purpose of better fault tolerance.</p>
<p><img src="/assets/images/2021-04-05-17-18-35.png"></p>
<h2 id="asynchronous-replication-overview"><a aria-hidden="true" class="anchor-heading" href="#asynchronous-replication-overview"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Asynchronous Replication Overview</h2>
<p><img src="/assets/images/2021-04-05-17-22-52.png"></p>
<p><img src="/assets/images/2021-04-05-17-23-23.png"></p></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#zippydb" title="Zippydb">Zippydb</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#arch" title="Arch">Arch</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#write" title="Write">Write</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#read" title="Read">Read</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#primary-failover" title="Primary Failover">Primary Failover</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#a-failed-replica-rejoins" title="A Failed Replica Rejoins">A Failed Replica Rejoins</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#sharding-and-load-balancing" title="Sharding And Load Balancing">Sharding And Load Balancing</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#asynchronous-replication-overview" title="Asynchronous Replication Overview">Asynchronous Replication Overview</a></div></div></div></div></div></div></div></div></div></main><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></section></section></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"40bf8bae-6748-4638-a9ea-43213edc6fb7","title":"Zippydb","vault":{"fsPath":"repos/dendron-aws-vault","remote":{"type":"git","url":"https://github.com/cczhong11/my_note.git"},"name":"my_note"},"type":"note","desc":"","links":[],"anchors":{"zippydb":{"type":"header","value":"zippydb","line":8,"column":0},"arch":{"type":"header","value":"arch","line":14,"column":0},"write":{"type":"header","value":"write","line":22,"column":0},"read":{"type":"header","value":"read","line":26,"column":0},"primary-failover":{"type":"header","value":"primary-failover","line":30,"column":0},"a-failed-replica-rejoins":{"type":"header","value":"a-failed-replica-rejoins","line":47,"column":0},"sharding-and-load-balancing":{"type":"header","value":"sharding-and-load-balancing","line":53,"column":0},"asynchronous-replication-overview":{"type":"header","value":"asynchronous-replication-overview","line":60,"column":0}},"fname":"development.database.zippydb","updated":1617668655908,"created":1617667853401,"parent":"a40ef849-d301-4d74-a778-e6d9469dfb5d","children":[],"data":{},"custom":{},"contentHash":"8c9c99a1bef06f21d1986f95eda0f051"},"body":"\u003ch1 id=\"zippydb\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#zippydb\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eZippydb\u003c/h1\u003e\n\u003ch1 id=\"zippydb-1\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#zippydb-1\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003ezippydb\u003c/h1\u003e\n\u003cp\u003efb internal distributed key-value databse\u003c/p\u003e\n\u003cp\u003esingle node based on rockdb\u003c/p\u003e\n\u003ch2 id=\"arch\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#arch\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003earch\u003c/h2\u003e\n\u003cp\u003eZippyDB servers can be mapped to the roles in the Multi-Paxos protocol as follows:\u003c/p\u003e\n\u003cp\u003ePrimary server = proposer/leader + acceptor + learner\nSecondary server = acceptor + learner\nFollower server = learner (An ordinary ZippyDB user may safely skip the details of the mapping above, as it is only intended to map terminology for readers who are familiar with Paxos.)\u003c/p\u003e\n\u003ch2 id=\"write\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#write\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003ewrite\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2021-04-05-17-12-35.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"read\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#read\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eread\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2021-04-05-17-13-08.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"primary-failover\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#primary-failover\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eprimary failover\u003c/h2\u003e\n\u003cp\u003eWhen the primary fails, Zeus detects the failure through lost heartbeats and notifies ShardManager. ShardManager chooses a most appropriate secondary and sends it a message to convert it into a new primary. \u003c/p\u003e\n\u003cp\u003eSpecifically in this example, it is possible that, right before server X crashed, server X and server Y worked together to accept a write and server X has already sent the \"write-success\" response back to the client, but that write has not yet reached server Z. This is because server X and server Y form a majority and hence can accept a write without waiting for the \"accepted\" confirmation from server Z. Actually, server Z may never get the \"accept\" notification for this write from server X before server X crashed, e.g., due to slow network between server X and server Z. In this case, after server Z becomes the new primary, it needs to contact server Y to recover the missing write. The prepare phase of the Paxos protocol guarantees that the new primary will correctly discover all those missing writes, so long as only a minority of the replicas have failed.\nAfter bringing its local replica up-to-date, the new primary (server Z in this example) starts to handle reads and writes as normal. As shown in the figure below, the write path executes the following steps in sequence:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eWhen the client wants to send a new \"Put()\" request, the ServiceRouter library linked into the client notices that ShardManager has changed the shard's primary assignment, and automatically routes the request to the new primary (server Z).\u003c/li\u003e\n\u003cli\u003eServer Z asks both server X and server Y to \"accept\" the write.\u003c/li\u003e\n\u003cli\u003eServer Z gets the \"accepted\" confirmation from server Y, but gets no confirmation from server X.\u003c/li\u003e\n\u003cli\u003eSince server Z collects a majority votes for the write (one from itself and another from server Y), server Z decides to commit the write and sends a \"write-success\" response back to the client. Server Z does not wait for server X to recover in order to accept the write. In other words, once server Z becomes the new primary, the failure of server X does not affect the database's availability .\u003c/li\u003e\n\u003cli\u003eServer Z sends a \"commit\" message to server Y to inform server Y that an agreement has been reached on the write. This step is not shown in the figure for brevity.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2021-04-05-17-15-33.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThe process of primary fail-over takes about 10 seconds, including the time for Zeus to detect lost heartbeats from the old primary\u003c/p\u003e\n\u003ch2 id=\"a-failed-replica-rejoins\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#a-failed-replica-rejoins\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eA Failed Replica Rejoins\u003c/h2\u003e\n\u003cp\u003eServer X synchronizes with the new primary (server Z) to bring its local database up-to-date. It then starts to process protocol messages as normal. See the example in the figure below.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2021-04-05-17-16-52.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"sharding-and-load-balancing\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#sharding-and-load-balancing\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eSharding and Load Balancing\u003c/h2\u003e\n\u003cp\u003eIn practice, each server runs a single ZippyDB process that can host multiple shards. In the figure below, each server hosts three shards: a primary for one shard, and two secondaries for two other shards. For example, server 1 hosts shard A's primary, shard D's secondary, and shard E's secondary. The three replicas of shard A are distributed across server 1 (primary), server 2 (secondary), and server 5 (secondary). ShardManager considers multiple factors in shard placement. In this figure, ShardManager places exactly one primary on each server for the purpose of load balancing, because a primary incurs a higher load than a secondary does. ShardManager may also place the different replicas of a shard across different clusters for the purpose of better fault tolerance.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2021-04-05-17-18-35.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"asynchronous-replication-overview\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#asynchronous-replication-overview\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eAsynchronous Replication Overview\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2021-04-05-17-22-52.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2021-04-05-17-23-23.png\"\u003e\u003c/p\u003e","noteIndex":{"id":"0f1b48c7-3a25-4016-83a5-15864d7803ad","title":"Development","vault":{"fsPath":"repos/dendron-aws-vault","remote":{"type":"git","url":"https://github.com/cczhong11/my_note.git"},"name":"my_note"},"type":"note","desc":"","links":[{"from":{"fname":"root","vaultName":"my_note"},"type":"backlink","position":{"start":{"line":8,"column":3,"offset":63},"end":{"line":8,"column":21,"offset":81},"indent":[]},"value":"development","alias":"开发"}],"anchors":{},"fname":"development","updated":1618381238346,"created":1612940782409,"parent":null,"children":["400115e9-30f7-4a13-9776-db059bc9cd42","xsX5v3ZsyJ0i6gf9","1bc3b45a-b6f4-4150-87d3-5bd5b6eb8c24","6cea4852-6e7c-4140-b476-85c07b48a642","a40ef849-d301-4d74-a778-e6d9469dfb5d","09d9081f-3dff-453d-8488-7d2344cc8895","92917ea3-452e-48dc-875e-5cd0002041db","2e151826-cb22-4d89-8ce0-71dad7204ce8","baa39444-0da9-4c55-8df7-2a6f8f787fa4","62daf50d-a39e-463f-aabd-be53790281fd","3524d0a7-be73-45d6-847e-c970f5c1c760","eac0f243-05b3-4b95-bec3-848e33edbc40","ocei7tftc6l4ea0165ee8rx"],"data":{},"custom":{"nav_order":0,"permalink":"/"},"body":"\n","contentHash":"38357962f5a50c6fd7d318dfa66bea90"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"previewV2Enabled":false,"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{}},"workspace":{"dendronVersion":"0.90.0","vaults":[{"fsPath":"repos/dendron-aws-vault","remote":{"type":"git","url":"https://github.com/cczhong11/my_note.git"},"name":"my_note"},{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableUserTags":true,"enableHashTags":true,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"},"enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"automaticallyShowPreview":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["development","life"],"enableSiteLastModified":true,"siteRootDir":"docs","siteUrl":"https://notes.tczhong.com","enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"writeStubs":false,"seo":{"title":"my_note","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enablePrettyLinks":true,"siteFaviconPath":"favicon.ico","siteIndex":"development"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"40bf8bae-6748-4638-a9ea-43213edc6fb7"},"buildId":"BJJu79SH0O6jPLyBmlOdc","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>
<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><title>Performance</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal knowledge space"/><meta property="og:title" content="Performance"/><meta property="og:description" content="Personal knowledge space"/><meta property="og:url" content="https://notes.tczhong.com/notes/7ce478ac-4713-40e9-b8dd-1a45bc1c7f9d/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="5/3/2021"/><meta property="article:modified_time" content="5/13/2021"/><link rel="canonical" href="https://notes.tczhong.com/notes/7ce478ac-4713-40e9-b8dd-1a45bc1c7f9d/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/_next/static/css/35c8e98bba5d4075.css" as="style"/><link rel="stylesheet" href="/_next/static/css/35c8e98bba5d4075.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-378e68e29c265886.js" defer=""></script><script src="/_next/static/chunks/framework-dc33c0b5493501f0.js" defer=""></script><script src="/_next/static/chunks/main-a7fe8c0ea9a0f989.js" defer=""></script><script src="/_next/static/chunks/pages/_app-fdb9297d37c44607.js" defer=""></script><script src="/_next/static/chunks/155-4c900bf5c5278e05.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bid%5D-ebe33fbe56ae1215.js" defer=""></script><script src="/_next/static/qHbm4vS5s7hFBo6UvcS91/_buildManifest.js" defer=""></script><script src="/_next/static/qHbm4vS5s7hFBo6UvcS91/_ssgManifest.js" defer=""></script><script src="/_next/static/qHbm4vS5s7hFBo6UvcS91/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div class="ant-col"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"></div><div style="margin-left:4px;display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout site-layout" style="margin-top:64px"><section class="ant-layout site-layout" style="flex-direction:row"><section class="ant-layout site-layout-sidebar" style="flex:0 0 auto;width:calc((100% - 992px) / 2 + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></section><section class="ant-layout side-layout-main" style="max-width:960px;display:initial"><main class="ant-layout-content main-content" role="main" style="padding:0 24px"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-20"><div><h1 id="performance"><a aria-hidden="true" class="anchor-heading" href="#performance"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Performance</h1>
<p><img src="/assets/images/2021-05-03-16-22-48.png">
<a href="http://www.brendangregg.com/linuxperf.html">reference</a></p>
<h2 id="1-uptime"><a aria-hidden="true" class="anchor-heading" href="#1-uptime"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>1. uptime</h2>
<pre><code>$ uptime 
23:51:26 up 21:31, 1 user, load average: 30.02, 26.43, 19.02
</code></pre>
<p>This is a quick way to view the load averages, which indicate the number of tasks (processes) wanting to run. On Linux systems, these numbers include processes wanting to run on CPU, as well as processes blocked in uninterruptible I/O (usually disk I/O). This gives a high level idea of resource load (or demand), but can’t be properly understood without other tools. Worth a quick look only.
The <strong>three numbers are exponentially damped moving sum averages with a 1 minute, 5 minute, and 15 minute constant</strong>. The three numbers give us some idea of how load is changing over time. For example, if you’ve been asked to check a problem server, and the 1 minute value is much lower than the 15 minute value, then you might have logged in too late and missed the issue.
In the example above, the load averages show a recent increase, hitting 30 for the 1 minute value, compared to 19 for the 15 minute value. That the numbers are this large means a lot of something: probably CPU demand; vmstat or mpstat will confirm, which are commands 3 and 4 in this sequence.</p>
<h2 id="2-dmesg--tail"><a aria-hidden="true" class="anchor-heading" href="#2-dmesg--tail"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>2. dmesg | tail</h2>
<pre><code>$ dmesg | tail
[1880957.563150] perl invoked oom-killer: gfp_mask=0x280da, order=0, oom_score_adj=0
[...]
[1880957.563400] Out of memory: Kill process 18694 (perl) score 246 or sacrifice child
[1880957.563408] Killed process 18694 (perl) total-vm:1972392kB, anon-rss:1953348kB, file-rss:0kB
[2320864.954447] TCP: Possible SYN flooding on port 7001. Dropping request.  Check SNMP counters.
</code></pre>
<p>This views the last 10 system messages, if there are any. <strong>Look for errors that can cause performance issues</strong>. The example above includes the oom-killer, and TCP dropping a request.
Don’t miss this step! dmesg is always worth checking.</p>
<p>kernel会将开机信息存储在ring buffer中.您若是开机时来不及查看信息，可利用dmesg来查看。开机信息亦保存在/var/log目录中，名称为dmesg的文件里。</p>
<h2 id="3-vmstat-1"><a aria-hidden="true" class="anchor-heading" href="#3-vmstat-1"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>3. vmstat 1</h2>
<pre><code>$ vmstat 1
procs ---------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
34  0    0 200889792  73708 591828    0    0     0     5    6   10 96  1  3  0  0
32  0    0 200889920  73708 591860    0    0     0   592 13284 4282 98  1  1  0  0
32  0    0 200890112  73708 591860    0    0     0     0 9501 2154 99  1  0  0  0
32  0    0 200889568  73712 591856    0    0     0    48 11900 2459 99  0  0  0  0
32  0    0 200890208  73712 591860    0    0     0     0 15898 4840 98  1  1  0  0
</code></pre>
<p>Short for virtual memory stat, vmstat(8) is a commonly available tool (first created for BSD decades ago). It prints a summary of key server statistics on each line.</p>
<p>vmstat was run with an argument of 1, <strong>to print one second summaries</strong>. The first line of output (in this version of vmstat) has some columns that show the average since boot, instead of the previous second. For now, skip the first line, unless you want to learn and remember which column is which.
Columns to check:
r: <strong>Number of processes running on CPU and waiting for a turn.</strong> This provides a better signal than load averages for determining CPU saturation, as it does not include I/O. To interpret: an “r” value greater than the CPU count is saturation.
free: <strong>Free memory in kilobytes.</strong> If there are too many digits to count, you have enough free memory. The “free -m” command, included as command 7, better explains the state of free memory.
si, so: Swap-ins and swap-outs. <strong>If these are non-zero, you’re out of memory</strong>.
us, sy, id, wa, st: These are breakdowns of CPU time, on average across all CPUs. <strong>They are user time, system time (kernel), idle, wait I/O, and stolen time</strong> (by other guests, or with Xen, the guest’s own isolated driver domain).
The CPU time breakdowns will confirm if the CPUs are busy, by adding user + system time. A constant degree of wait I/O points to a disk bottleneck; this is where the CPUs are idle, because tasks are blocked waiting for pending disk I/O. You can treat wait I/O as another form of CPU idle, one that gives a clue as to why they are idle.
System time is necessary for I/O processing. A high system time average, over 20%, can be interesting to explore further: perhaps the kernel is processing the I/O inefficiently.
In the above example, CPU time is almost entirely in user-level, pointing to application level usage instead. The CPUs are also well over 90% utilized on average. This isn’t necessarily a problem; check for the degree of saturation using the “r” column.</p>
<h2 id="4-mpstat--p-all-1"><a aria-hidden="true" class="anchor-heading" href="#4-mpstat--p-all-1"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>4. mpstat -P ALL 1</h2>
<pre><code>$ mpstat -P ALL 1
Linux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015  _x86_64_ (32 CPU)

07:38:49 PM  CPU   %usr  %nice   %sys %iowait   %irq  %soft  %steal  %guest  %gnice  %idle
07:38:50 PM  all  98.47   0.00   0.75    0.00   0.00   0.00    0.00    0.00    0.00   0.78
07:38:50 PM    0  96.04   0.00   2.97    0.00   0.00   0.00    0.00    0.00    0.00   0.99
07:38:50 PM    1  97.00   0.00   1.00    0.00   0.00   0.00    0.00    0.00    0.00   2.00
07:38:50 PM    2  98.00   0.00   1.00    0.00   0.00   0.00    0.00    0.00    0.00   1.00
07:38:50 PM    3  96.97   0.00   0.00    0.00   0.00   0.00    0.00    0.00    0.00   3.03
[...]
</code></pre>
<p>This command prints CPU time breakdowns per CPU, which can be used to <strong>check for an imbalance</strong>. A single hot CPU can be evidence of a single-threaded application.</p>
<h2 id="5-pidstat-1"><a aria-hidden="true" class="anchor-heading" href="#5-pidstat-1"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>5. pidstat 1</h2>
<pre><code>$ pidstat 1
Linux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015    _x86_64_    (32 CPU)

07:41:02 PM   UID       PID    %usr %system  %guest    %CPU   CPU  Command
07:41:03 PM     0         9    0.00    0.94    0.00    0.94     1  rcuos/0
07:41:03 PM     0      4214    5.66    5.66    0.00   11.32    15  mesos-slave
07:41:03 PM     0      4354    0.94    0.94    0.00    1.89     8  java
07:41:03 PM     0      6521 1596.23    1.89    0.00 1598.11    27  java
07:41:03 PM     0      6564 1571.70    7.55    0.00 1579.25    28  java
07:41:03 PM 60004     60154    0.94    4.72    0.00    5.66     9  pidstat

07:41:03 PM   UID       PID    %usr %system  %guest    %CPU   CPU  Command
07:41:04 PM     0      4214    6.00    2.00    0.00    8.00    15  mesos-slave
07:41:04 PM     0      6521 1590.00    1.00    0.00 1591.00    27  java
07:41:04 PM     0      6564 1573.00   10.00    0.00 1583.00    28  java
07:41:04 PM   108      6718    1.00    0.00    0.00    1.00     0  snmp-pass
07:41:04 PM 60004     60154    1.00    4.00    0.00    5.00     9  pidstat
</code></pre>
<p>Pidstat is a little like top’s per-process summary, but prints a rolling summary instead of clearing the screen. This can be useful for watching patterns over time, and also recording what you saw (copy-n-paste) into a record of your investigation.
The above example identifies two java processes as responsible for consuming CPU. The %CPU column is the total across all CPUs; 1591% shows that that java processes is consuming almost 16 CPUs.</p>
<h2 id="6-iostat--xz-1"><a aria-hidden="true" class="anchor-heading" href="#6-iostat--xz-1"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>6. iostat -xz 1</h2>
<pre><code>$ iostat -xz 1
Linux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015  _x86_64_ (32 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          73.96    0.00    3.73    0.03    0.06   22.21

Device:   rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
xvda        0.00     0.23    0.21    0.18     4.52     2.08    34.37     0.00    9.98   13.80    5.42   2.44   0.09
xvdb        0.01     0.00    1.02    8.94   127.97   598.53   145.79     0.00    0.43    1.78    0.28   0.25   0.25
xvdc        0.01     0.00    1.02    8.86   127.79   595.94   146.50     0.00    0.45    1.82    0.30   0.27   0.26
dm-0        0.00     0.00    0.69    2.32    10.47    31.69    28.01     0.01    3.23    0.71    3.98   0.13   0.04
dm-1        0.00     0.00    0.00    0.94     0.01     3.78     8.00     0.33  345.84    0.04  346.81   0.01   0.00
dm-2        0.00     0.00    0.09    0.07     1.35     0.36    22.50     0.00    2.55    0.23    5.62   1.78   0.03
[...]
</code></pre>
<p>This is a great tool for understanding <strong>block devices</strong> (disks), both the workload applied and the resulting performance. Look for:
r/s, w/s, rkB/s, wkB/s: These are the delivered reads, writes, read Kbytes, and write Kbytes per second to the device. Use these for workload characterization. <strong>A performance problem may simply be due to an excessive load applied.</strong>
await: The average time for the I/O in milliseconds. This is the time that the application suffers, as it includes both time queued and time being serviced. <strong>Larger than expected average times can be an indicator of device saturation, or device problems.</strong>
avgqu-sz: The average number of requests issued to the device. Values greater than 1 can be evidence of saturation (although devices can typically operate on requests in parallel, especially virtual devices which front multiple back-end disks.)
%util: Device utilization. This is really a busy percent, showing the time each second that the device was doing work. Values greater than 60% typically lead to poor performance (which should be seen in await), although it depends on the device. Values close to 100% usually indicate saturation.
If the storage device is a logical disk device fronting many back-end disks, then 100% utilization may just mean that some I/O is being processed 100% of the time, however, the back-end disks may be far from saturated, and may be able to handle much more work.
Bear in mind that poor performing disk I/O isn’t necessarily an application issue. Many techniques are typically used to perform I/O asynchronously, so that the application doesn’t block and suffer the latency directly (e.g., read-ahead for reads, and buffering for writes).</p>
<h2 id="7-free--m"><a aria-hidden="true" class="anchor-heading" href="#7-free--m"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>7. free -m</h2>
<pre><code>$ free -m
             total       used       free     shared    buffers     cached
Mem:        245998      24545     221453         83         59        541
-/+ buffers/cache:      23944     222053
Swap:            0          0          0
</code></pre>
<p>The right two columns show:
buffers: <strong>For the buffer cache, used for block device I/O.</strong>
cached: <strong>For the page cache, used by file systems</strong>.
We just want to check that these aren’t near-zero in size, which can lead to higher disk I/O (confirm using iostat), and worse performance. The above example looks fine, with many Mbytes in each.
The “-/+ buffers/cache” provides less confusing values for used and free memory. Linux uses free memory for the caches, but can reclaim it quickly if applications need it. So in a way the cached memory should be included in the free memory column, which this line does. There’s even a website, linuxatemyram, about this confusion.
It can be additionally confusing if ZFS on Linux is used, as we do for some services, as ZFS has its own file system cache that isn’t reflected properly by the free -m columns. It can appear that the system is low on free memory, when that memory is in fact available for use from the ZFS cache as needed.</p>
<h2 id="8-sar--n-dev-1"><a aria-hidden="true" class="anchor-heading" href="#8-sar--n-dev-1"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>8. sar -n DEV 1</h2>
<pre><code>$ sar -n DEV 1
Linux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015     _x86_64_    (32 CPU)

12:16:48 AM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
12:16:49 AM      eth0  18763.00   5032.00  20686.42    478.30      0.00      0.00      0.00      0.00
12:16:49 AM        lo     14.00     14.00      1.36      1.36      0.00      0.00      0.00      0.00
12:16:49 AM   docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00

12:16:49 AM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
12:16:50 AM      eth0  19763.00   5101.00  21999.10    482.56      0.00      0.00      0.00      0.00
12:16:50 AM        lo     20.00     20.00      3.25      3.25      0.00      0.00      0.00      0.00
12:16:50 AM   docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
^C
</code></pre>
<p>Use this tool to check <strong>network interface throughput</strong>: rxkB/s and txkB/s, as a measure of workload, and also to check if any limit has been reached. In the above example, eth0 receive is reaching 22 Mbytes/s, which is 176 Mbits/sec (well under, say, a 1 Gbit/sec limit).
This version also has %ifutil for device utilization (max of both directions for full duplex), which is something we also use Brendan’s nicstat tool to measure. And like with nicstat, this is hard to get right, and seems to not be working in this example (0.00).</p>
<h2 id="9-sar--n-tcpetcp-1"><a aria-hidden="true" class="anchor-heading" href="#9-sar--n-tcpetcp-1"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>9. sar -n TCP,ETCP 1</h2>
<pre><code>$ sar -n TCP,ETCP 1
Linux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015    _x86_64_    (32 CPU)

12:17:19 AM  active/s passive/s    iseg/s    oseg/s
12:17:20 AM      1.00      0.00  10233.00  18846.00

12:17:19 AM  atmptf/s  estres/s retrans/s isegerr/s   orsts/s
12:17:20 AM      0.00      0.00      0.00      0.00      0.00

12:17:20 AM  active/s passive/s    iseg/s    oseg/s
12:17:21 AM      1.00      0.00   8359.00   6039.00

12:17:20 AM  atmptf/s  estres/s retrans/s isegerr/s   orsts/s
12:17:21 AM      0.00      0.00      0.00      0.00      0.00
^C
</code></pre>
<p>This is a summarized view of some key TCP metrics. These include:
active/s: Number of locally-initiated TCP connections per second (e.g., via connect()).
passive/s: Number of remotely-initiated TCP connections per second (e.g., via accept()).
retrans/s: Number of TCP retransmits per second.
The active and passive counts are often useful as a rough measure of server load: number of new accepted connections (passive), and number of downstream connections (active). It might help to think of active as outbound, and passive as inbound, but this isn’t strictly true (e.g., consider a localhost to localhost connection).
Retransmits are a sign of a network or server issue; it may be an unreliable network (e.g., the public Internet), or it may be due a server being overloaded and dropping packets. The example above shows just one new TCP connection per-second.</p>
<h2 id="10-top"><a aria-hidden="true" class="anchor-heading" href="#10-top"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>10. top</h2>
<pre><code>$ top
top - 00:15:40 up 21:56,  1 user,  load average: 31.09, 29.87, 29.92
Tasks: 871 total,   1 running, 868 sleeping,   0 stopped,   2 zombie
%Cpu(s): 96.8 us,  0.4 sy,  0.0 ni,  2.7 id,  0.1 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem:  25190241+total, 24921688 used, 22698073+free,    60448 buffers
KiB Swap:        0 total,        0 used,        0 free.   554208 cached Mem

   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 20248 root      20   0  0.227t 0.012t  18748 S  3090  5.2  29812:58 java
  4213 root      20   0 2722544  64640  44232 S  23.5  0.0 233:35.37 mesos-slave
 66128 titancl+  20   0   24344   2332   1172 R   1.0  0.0   0:00.07 top
  5235 root      20   0 38.227g 547004  49996 S   0.7  0.2   2:02.74 java
  4299 root      20   0 20.015g 2.682g  16836 S   0.3  1.1  33:14.42 java
     1 root      20   0   33620   2920   1496 S   0.0  0.0   0:03.82 init
     2 root      20   0       0      0      0 S   0.0  0.0   0:00.02 kthreadd
     3 root      20   0       0      0      0 S   0.0  0.0   0:05.35 ksoftirqd/0
     5 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H
     6 root      20   0       0      0      0 S   0.0  0.0   0:06.94 kworker/u256:0
     8 root      20   0       0      0      0 S   0.0  0.0   2:38.05 rcu_sched
</code></pre>
<p>The top command includes many of the metrics we checked earlier. It can be handy to run it to see if anything looks wildly different from the earlier commands, which would indicate that load is variable.</p>
<h2 id="strace"><a aria-hidden="true" class="anchor-heading" href="#strace"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>strace</h2>
<p> trace system calls and signals</p>
<h2 id="iotop"><a aria-hidden="true" class="anchor-heading" href="#iotop"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>iotop</h2></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-4"><div><div class=""><div class="ant-anchor-wrapper" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#1-uptime" title="1 Uptime">1 Uptime</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#2-dmesg--tail" title="2 Dmesg Tail">2 Dmesg Tail</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#3-vmstat-1" title="3 Vmstat 1">3 Vmstat 1</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#4-mpstat--p-all-1" title="4 Mpstat P All 1">4 Mpstat P All 1</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#5-pidstat-1" title="5 Pidstat 1">5 Pidstat 1</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#6-iostat--xz-1" title="6 Iostat Xz 1">6 Iostat Xz 1</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#7-free--m" title="7 Free M">7 Free M</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#8-sar--n-dev-1" title="8 Sar N Dev 1">8 Sar N Dev 1</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#9-sar--n-tcpetcp-1" title="9 Sar N Tcpetcp 1">9 Sar N Tcpetcp 1</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#10-top" title="10 Top">10 Top</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#strace" title="Strace">Strace</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#iotop" title="Iotop">Iotop</a></div></div></div></div></div></div></div></div></div></main><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></section></section></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"7ce478ac-4713-40e9-b8dd-1a45bc1c7f9d","title":"Performance","vault":{"fsPath":"repos/dendron-aws-vault","remote":{"type":"git","url":"https://github.com/cczhong11/my_note.git"},"name":"my_note"},"type":"note","desc":"","links":[],"anchors":{"1-uptime":{"type":"header","value":"1-uptime","line":11,"column":0},"2-dmesg--tail":{"type":"header","value":"2-dmesg--tail","line":22,"column":0},"3-vmstat-1":{"type":"header","value":"3-vmstat-1","line":40,"column":0},"4-mpstat--p-all-1":{"type":"header","value":"4-mpstat--p-all-1","line":65,"column":0},"5-pidstat-1":{"type":"header","value":"5-pidstat-1","line":82,"column":0},"6-iostat--xz-1":{"type":"header","value":"6-iostat--xz-1","line":107,"column":0},"7-free--m":{"type":"header","value":"7-free--m","line":133,"column":0},"8-sar--n-dev-1":{"type":"header","value":"8-sar--n-dev-1","line":150,"column":0},"9-sar--n-tcpetcp-1":{"type":"header","value":"9-sar--n-tcpetcp-1","line":172,"column":0},"10-top":{"type":"header","value":"10-top","line":199,"column":0},"strace":{"type":"header","value":"strace","line":226,"column":0},"iotop":{"type":"header","value":"iotop","line":230,"column":0}},"fname":"development.system.performance","updated":1620948525539,"created":1620084110638,"parent":"62daf50d-a39e-463f-aabd-be53790281fd","children":[],"data":{},"custom":{},"contentHash":"78fb154cf7428a69a9bfffe0f5a2dc2d"},"body":"\u003ch1 id=\"performance\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#performance\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003ePerformance\u003c/h1\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2021-05-03-16-22-48.png\"\u003e\n\u003ca href=\"http://www.brendangregg.com/linuxperf.html\"\u003ereference\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"1-uptime\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#1-uptime\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e1. uptime\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e$ uptime \n23:51:26 up 21:31, 1 user, load average: 30.02, 26.43, 19.02\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is a quick way to view the load averages, which indicate the number of tasks (processes) wanting to run. On Linux systems, these numbers include processes wanting to run on CPU, as well as processes blocked in uninterruptible I/O (usually disk I/O). This gives a high level idea of resource load (or demand), but can’t be properly understood without other tools. Worth a quick look only.\nThe \u003cstrong\u003ethree numbers are exponentially damped moving sum averages with a 1 minute, 5 minute, and 15 minute constant\u003c/strong\u003e. The three numbers give us some idea of how load is changing over time. For example, if you’ve been asked to check a problem server, and the 1 minute value is much lower than the 15 minute value, then you might have logged in too late and missed the issue.\nIn the example above, the load averages show a recent increase, hitting 30 for the 1 minute value, compared to 19 for the 15 minute value. That the numbers are this large means a lot of something: probably CPU demand; vmstat or mpstat will confirm, which are commands 3 and 4 in this sequence.\u003c/p\u003e\n\u003ch2 id=\"2-dmesg--tail\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#2-dmesg--tail\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e2. dmesg | tail\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e$ dmesg | tail\n[1880957.563150] perl invoked oom-killer: gfp_mask=0x280da, order=0, oom_score_adj=0\n[...]\n[1880957.563400] Out of memory: Kill process 18694 (perl) score 246 or sacrifice child\n[1880957.563408] Killed process 18694 (perl) total-vm:1972392kB, anon-rss:1953348kB, file-rss:0kB\n[2320864.954447] TCP: Possible SYN flooding on port 7001. Dropping request.  Check SNMP counters.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis views the last 10 system messages, if there are any. \u003cstrong\u003eLook for errors that can cause performance issues\u003c/strong\u003e. The example above includes the oom-killer, and TCP dropping a request.\nDon’t miss this step! dmesg is always worth checking.\u003c/p\u003e\n\u003cp\u003ekernel会将开机信息存储在ring buffer中.您若是开机时来不及查看信息，可利用dmesg来查看。开机信息亦保存在/var/log目录中，名称为dmesg的文件里。\u003c/p\u003e\n\u003ch2 id=\"3-vmstat-1\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#3-vmstat-1\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e3. vmstat 1\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e$ vmstat 1\nprocs ---------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n34  0    0 200889792  73708 591828    0    0     0     5    6   10 96  1  3  0  0\n32  0    0 200889920  73708 591860    0    0     0   592 13284 4282 98  1  1  0  0\n32  0    0 200890112  73708 591860    0    0     0     0 9501 2154 99  1  0  0  0\n32  0    0 200889568  73712 591856    0    0     0    48 11900 2459 99  0  0  0  0\n32  0    0 200890208  73712 591860    0    0     0     0 15898 4840 98  1  1  0  0\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eShort for virtual memory stat, vmstat(8) is a commonly available tool (first created for BSD decades ago). It prints a summary of key server statistics on each line.\u003c/p\u003e\n\u003cp\u003evmstat was run with an argument of 1, \u003cstrong\u003eto print one second summaries\u003c/strong\u003e. The first line of output (in this version of vmstat) has some columns that show the average since boot, instead of the previous second. For now, skip the first line, unless you want to learn and remember which column is which.\nColumns to check:\nr: \u003cstrong\u003eNumber of processes running on CPU and waiting for a turn.\u003c/strong\u003e This provides a better signal than load averages for determining CPU saturation, as it does not include I/O. To interpret: an “r” value greater than the CPU count is saturation.\nfree: \u003cstrong\u003eFree memory in kilobytes.\u003c/strong\u003e If there are too many digits to count, you have enough free memory. The “free -m” command, included as command 7, better explains the state of free memory.\nsi, so: Swap-ins and swap-outs. \u003cstrong\u003eIf these are non-zero, you’re out of memory\u003c/strong\u003e.\nus, sy, id, wa, st: These are breakdowns of CPU time, on average across all CPUs. \u003cstrong\u003eThey are user time, system time (kernel), idle, wait I/O, and stolen time\u003c/strong\u003e (by other guests, or with Xen, the guest’s own isolated driver domain).\nThe CPU time breakdowns will confirm if the CPUs are busy, by adding user + system time. A constant degree of wait I/O points to a disk bottleneck; this is where the CPUs are idle, because tasks are blocked waiting for pending disk I/O. You can treat wait I/O as another form of CPU idle, one that gives a clue as to why they are idle.\nSystem time is necessary for I/O processing. A high system time average, over 20%, can be interesting to explore further: perhaps the kernel is processing the I/O inefficiently.\nIn the above example, CPU time is almost entirely in user-level, pointing to application level usage instead. The CPUs are also well over 90% utilized on average. This isn’t necessarily a problem; check for the degree of saturation using the “r” column.\u003c/p\u003e\n\u003ch2 id=\"4-mpstat--p-all-1\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#4-mpstat--p-all-1\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e4. mpstat -P ALL 1\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e$ mpstat -P ALL 1\nLinux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015  _x86_64_ (32 CPU)\n\n07:38:49 PM  CPU   %usr  %nice   %sys %iowait   %irq  %soft  %steal  %guest  %gnice  %idle\n07:38:50 PM  all  98.47   0.00   0.75    0.00   0.00   0.00    0.00    0.00    0.00   0.78\n07:38:50 PM    0  96.04   0.00   2.97    0.00   0.00   0.00    0.00    0.00    0.00   0.99\n07:38:50 PM    1  97.00   0.00   1.00    0.00   0.00   0.00    0.00    0.00    0.00   2.00\n07:38:50 PM    2  98.00   0.00   1.00    0.00   0.00   0.00    0.00    0.00    0.00   1.00\n07:38:50 PM    3  96.97   0.00   0.00    0.00   0.00   0.00    0.00    0.00    0.00   3.03\n[...]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis command prints CPU time breakdowns per CPU, which can be used to \u003cstrong\u003echeck for an imbalance\u003c/strong\u003e. A single hot CPU can be evidence of a single-threaded application.\u003c/p\u003e\n\u003ch2 id=\"5-pidstat-1\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#5-pidstat-1\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e5. pidstat 1\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e$ pidstat 1\nLinux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015    _x86_64_    (32 CPU)\n\n07:41:02 PM   UID       PID    %usr %system  %guest    %CPU   CPU  Command\n07:41:03 PM     0         9    0.00    0.94    0.00    0.94     1  rcuos/0\n07:41:03 PM     0      4214    5.66    5.66    0.00   11.32    15  mesos-slave\n07:41:03 PM     0      4354    0.94    0.94    0.00    1.89     8  java\n07:41:03 PM     0      6521 1596.23    1.89    0.00 1598.11    27  java\n07:41:03 PM     0      6564 1571.70    7.55    0.00 1579.25    28  java\n07:41:03 PM 60004     60154    0.94    4.72    0.00    5.66     9  pidstat\n\n07:41:03 PM   UID       PID    %usr %system  %guest    %CPU   CPU  Command\n07:41:04 PM     0      4214    6.00    2.00    0.00    8.00    15  mesos-slave\n07:41:04 PM     0      6521 1590.00    1.00    0.00 1591.00    27  java\n07:41:04 PM     0      6564 1573.00   10.00    0.00 1583.00    28  java\n07:41:04 PM   108      6718    1.00    0.00    0.00    1.00     0  snmp-pass\n07:41:04 PM 60004     60154    1.00    4.00    0.00    5.00     9  pidstat\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePidstat is a little like top’s per-process summary, but prints a rolling summary instead of clearing the screen. This can be useful for watching patterns over time, and also recording what you saw (copy-n-paste) into a record of your investigation.\nThe above example identifies two java processes as responsible for consuming CPU. The %CPU column is the total across all CPUs; 1591% shows that that java processes is consuming almost 16 CPUs.\u003c/p\u003e\n\u003ch2 id=\"6-iostat--xz-1\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#6-iostat--xz-1\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e6. iostat -xz 1\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e$ iostat -xz 1\nLinux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015  _x86_64_ (32 CPU)\n\navg-cpu:  %user   %nice %system %iowait  %steal   %idle\n          73.96    0.00    3.73    0.03    0.06   22.21\n\nDevice:   rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util\nxvda        0.00     0.23    0.21    0.18     4.52     2.08    34.37     0.00    9.98   13.80    5.42   2.44   0.09\nxvdb        0.01     0.00    1.02    8.94   127.97   598.53   145.79     0.00    0.43    1.78    0.28   0.25   0.25\nxvdc        0.01     0.00    1.02    8.86   127.79   595.94   146.50     0.00    0.45    1.82    0.30   0.27   0.26\ndm-0        0.00     0.00    0.69    2.32    10.47    31.69    28.01     0.01    3.23    0.71    3.98   0.13   0.04\ndm-1        0.00     0.00    0.00    0.94     0.01     3.78     8.00     0.33  345.84    0.04  346.81   0.01   0.00\ndm-2        0.00     0.00    0.09    0.07     1.35     0.36    22.50     0.00    2.55    0.23    5.62   1.78   0.03\n[...]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is a great tool for understanding \u003cstrong\u003eblock devices\u003c/strong\u003e (disks), both the workload applied and the resulting performance. Look for:\nr/s, w/s, rkB/s, wkB/s: These are the delivered reads, writes, read Kbytes, and write Kbytes per second to the device. Use these for workload characterization. \u003cstrong\u003eA performance problem may simply be due to an excessive load applied.\u003c/strong\u003e\nawait: The average time for the I/O in milliseconds. This is the time that the application suffers, as it includes both time queued and time being serviced. \u003cstrong\u003eLarger than expected average times can be an indicator of device saturation, or device problems.\u003c/strong\u003e\navgqu-sz: The average number of requests issued to the device. Values greater than 1 can be evidence of saturation (although devices can typically operate on requests in parallel, especially virtual devices which front multiple back-end disks.)\n%util: Device utilization. This is really a busy percent, showing the time each second that the device was doing work. Values greater than 60% typically lead to poor performance (which should be seen in await), although it depends on the device. Values close to 100% usually indicate saturation.\nIf the storage device is a logical disk device fronting many back-end disks, then 100% utilization may just mean that some I/O is being processed 100% of the time, however, the back-end disks may be far from saturated, and may be able to handle much more work.\nBear in mind that poor performing disk I/O isn’t necessarily an application issue. Many techniques are typically used to perform I/O asynchronously, so that the application doesn’t block and suffer the latency directly (e.g., read-ahead for reads, and buffering for writes).\u003c/p\u003e\n\u003ch2 id=\"7-free--m\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#7-free--m\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e7. free -m\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e$ free -m\n             total       used       free     shared    buffers     cached\nMem:        245998      24545     221453         83         59        541\n-/+ buffers/cache:      23944     222053\nSwap:            0          0          0\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe right two columns show:\nbuffers: \u003cstrong\u003eFor the buffer cache, used for block device I/O.\u003c/strong\u003e\ncached: \u003cstrong\u003eFor the page cache, used by file systems\u003c/strong\u003e.\nWe just want to check that these aren’t near-zero in size, which can lead to higher disk I/O (confirm using iostat), and worse performance. The above example looks fine, with many Mbytes in each.\nThe “-/+ buffers/cache” provides less confusing values for used and free memory. Linux uses free memory for the caches, but can reclaim it quickly if applications need it. So in a way the cached memory should be included in the free memory column, which this line does. There’s even a website, linuxatemyram, about this confusion.\nIt can be additionally confusing if ZFS on Linux is used, as we do for some services, as ZFS has its own file system cache that isn’t reflected properly by the free -m columns. It can appear that the system is low on free memory, when that memory is in fact available for use from the ZFS cache as needed.\u003c/p\u003e\n\u003ch2 id=\"8-sar--n-dev-1\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#8-sar--n-dev-1\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e8. sar -n DEV 1\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e$ sar -n DEV 1\nLinux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015     _x86_64_    (32 CPU)\n\n12:16:48 AM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil\n12:16:49 AM      eth0  18763.00   5032.00  20686.42    478.30      0.00      0.00      0.00      0.00\n12:16:49 AM        lo     14.00     14.00      1.36      1.36      0.00      0.00      0.00      0.00\n12:16:49 AM   docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n\n12:16:49 AM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil\n12:16:50 AM      eth0  19763.00   5101.00  21999.10    482.56      0.00      0.00      0.00      0.00\n12:16:50 AM        lo     20.00     20.00      3.25      3.25      0.00      0.00      0.00      0.00\n12:16:50 AM   docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00\n^C\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eUse this tool to check \u003cstrong\u003enetwork interface throughput\u003c/strong\u003e: rxkB/s and txkB/s, as a measure of workload, and also to check if any limit has been reached. In the above example, eth0 receive is reaching 22 Mbytes/s, which is 176 Mbits/sec (well under, say, a 1 Gbit/sec limit).\nThis version also has %ifutil for device utilization (max of both directions for full duplex), which is something we also use Brendan’s nicstat tool to measure. And like with nicstat, this is hard to get right, and seems to not be working in this example (0.00).\u003c/p\u003e\n\u003ch2 id=\"9-sar--n-tcpetcp-1\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#9-sar--n-tcpetcp-1\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e9. sar -n TCP,ETCP 1\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e$ sar -n TCP,ETCP 1\nLinux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015    _x86_64_    (32 CPU)\n\n12:17:19 AM  active/s passive/s    iseg/s    oseg/s\n12:17:20 AM      1.00      0.00  10233.00  18846.00\n\n12:17:19 AM  atmptf/s  estres/s retrans/s isegerr/s   orsts/s\n12:17:20 AM      0.00      0.00      0.00      0.00      0.00\n\n12:17:20 AM  active/s passive/s    iseg/s    oseg/s\n12:17:21 AM      1.00      0.00   8359.00   6039.00\n\n12:17:20 AM  atmptf/s  estres/s retrans/s isegerr/s   orsts/s\n12:17:21 AM      0.00      0.00      0.00      0.00      0.00\n^C\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is a summarized view of some key TCP metrics. These include:\nactive/s: Number of locally-initiated TCP connections per second (e.g., via connect()).\npassive/s: Number of remotely-initiated TCP connections per second (e.g., via accept()).\nretrans/s: Number of TCP retransmits per second.\nThe active and passive counts are often useful as a rough measure of server load: number of new accepted connections (passive), and number of downstream connections (active). It might help to think of active as outbound, and passive as inbound, but this isn’t strictly true (e.g., consider a localhost to localhost connection).\nRetransmits are a sign of a network or server issue; it may be an unreliable network (e.g., the public Internet), or it may be due a server being overloaded and dropping packets. The example above shows just one new TCP connection per-second.\u003c/p\u003e\n\u003ch2 id=\"10-top\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#10-top\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e10. top\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e$ top\ntop - 00:15:40 up 21:56,  1 user,  load average: 31.09, 29.87, 29.92\nTasks: 871 total,   1 running, 868 sleeping,   0 stopped,   2 zombie\n%Cpu(s): 96.8 us,  0.4 sy,  0.0 ni,  2.7 id,  0.1 wa,  0.0 hi,  0.0 si,  0.0 st\nKiB Mem:  25190241+total, 24921688 used, 22698073+free,    60448 buffers\nKiB Swap:        0 total,        0 used,        0 free.   554208 cached Mem\n\n   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n 20248 root      20   0  0.227t 0.012t  18748 S  3090  5.2  29812:58 java\n  4213 root      20   0 2722544  64640  44232 S  23.5  0.0 233:35.37 mesos-slave\n 66128 titancl+  20   0   24344   2332   1172 R   1.0  0.0   0:00.07 top\n  5235 root      20   0 38.227g 547004  49996 S   0.7  0.2   2:02.74 java\n  4299 root      20   0 20.015g 2.682g  16836 S   0.3  1.1  33:14.42 java\n     1 root      20   0   33620   2920   1496 S   0.0  0.0   0:03.82 init\n     2 root      20   0       0      0      0 S   0.0  0.0   0:00.02 kthreadd\n     3 root      20   0       0      0      0 S   0.0  0.0   0:05.35 ksoftirqd/0\n     5 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H\n     6 root      20   0       0      0      0 S   0.0  0.0   0:06.94 kworker/u256:0\n     8 root      20   0       0      0      0 S   0.0  0.0   2:38.05 rcu_sched\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe top command includes many of the metrics we checked earlier. It can be handy to run it to see if anything looks wildly different from the earlier commands, which would indicate that load is variable.\u003c/p\u003e\n\u003ch2 id=\"strace\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#strace\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003estrace\u003c/h2\u003e\n\u003cp\u003e trace system calls and signals\u003c/p\u003e\n\u003ch2 id=\"iotop\"\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading\" href=\"#iotop\"\u003e\u003csvg aria-hidden=\"true\" viewBox=\"0 0 16 16\"\u003e\u003cuse xlink:href=\"#svg-link\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003eiotop\u003c/h2\u003e","noteIndex":{"id":"0f1b48c7-3a25-4016-83a5-15864d7803ad","title":"Development","vault":{"fsPath":"repos/dendron-aws-vault","remote":{"type":"git","url":"https://github.com/cczhong11/my_note.git"},"name":"my_note"},"type":"note","desc":"","links":[{"from":{"fname":"root","vaultName":"my_note"},"type":"backlink","position":{"start":{"line":8,"column":3,"offset":63},"end":{"line":8,"column":21,"offset":81},"indent":[]},"value":"development"}],"anchors":{},"fname":"development","updated":1618381238346,"created":1612940782409,"parent":null,"children":["400115e9-30f7-4a13-9776-db059bc9cd42","xsX5v3ZsyJ0i6gf9","1bc3b45a-b6f4-4150-87d3-5bd5b6eb8c24","6cea4852-6e7c-4140-b476-85c07b48a642","a40ef849-d301-4d74-a778-e6d9469dfb5d","09d9081f-3dff-453d-8488-7d2344cc8895","92917ea3-452e-48dc-875e-5cd0002041db","2e151826-cb22-4d89-8ce0-71dad7204ce8","baa39444-0da9-4c55-8df7-2a6f8f787fa4","62daf50d-a39e-463f-aabd-be53790281fd","3524d0a7-be73-45d6-847e-c970f5c1c760","eac0f243-05b3-4b95-bec3-848e33edbc40"],"data":{},"custom":{"nav_order":0,"permalink":"/"},"body":"\n","contentHash":"38357962f5a50c6fd7d318dfa66bea90"},"collectionChildren":null,"customHeadContent":null,"config":{"version":4,"useFMTitle":true,"useNoteTitleForLink":true,"mermaid":true,"useKatex":true,"dev":{"previewV2Enabled":false,"enablePreviewV2":true},"site":{"copyAssets":true,"siteHierarchies":["development","life"],"siteRootDir":"docs","usePrettyRefs":true,"title":"my_note","siteUrl":"https://notes.tczhong.com","description":"Personal knowledge space","siteLastModified":true,"gh_edit_branch":"main","usePrettyLinks":true,"siteNotesDir":"notes","siteFaviconPath":"favicon.ico","gh_edit_link":true,"gh_edit_link_text":"Edit this page on GitHub","gh_root":"docs/","gh_edit_view_mode":"edit","writeStubs":true,"siteIndex":"development"},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{}},"workspace":{"dendronVersion":"0.70.0","vaults":[{"fsPath":"repos/dendron-aws-vault","remote":{"type":"git","url":"https://github.com/cczhong11/my_note.git"},"name":"my_note"},{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableUserTags":true,"enableHashTags":true,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link"}},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"7ce478ac-4713-40e9-b8dd-1a45bc1c7f9d"},"buildId":"qHbm4vS5s7hFBo6UvcS91","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>
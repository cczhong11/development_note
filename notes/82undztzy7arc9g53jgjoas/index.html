<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.ico"/><title>tool</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal knowledge space"/><meta property="og:title" content="tool"/><meta property="og:description" content="Personal knowledge space"/><meta property="og:url" content="https://notes.tczhong.com/notes/82undztzy7arc9g53jgjoas/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="9/22/2023"/><meta property="article:modified_time" content="9/22/2023"/><link rel="canonical" href="https://notes.tczhong.com/notes/82undztzy7arc9g53jgjoas/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/_next/static/css/8e7b7e4bce421c0a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8e7b7e4bce421c0a.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-3d209faeb64f2f97.js" defer=""></script><script src="/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/_next/static/chunks/main-104451f3d1a5c4bc.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6b338472289fe290.js" defer=""></script><script src="/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/_next/static/2XI28xCTA7nnAuXRumXG6/_buildManifest.js" defer=""></script><script src="/_next/static/2XI28xCTA7nnAuXRumXG6/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="tool">tool<a aria-hidden="true" class="anchor-heading icon-link" href="#tool"></a></h1>
<h1 id="mrkl">MRKL<a aria-hidden="true" class="anchor-heading icon-link" href="#mrkl"></a></h1>
<p>MRKL（Karpas 等人，2022 年）是 "模块化推理、知识和语言 "的简称，是一种用于自主代理的神经符号架构。建议 MRKL 系统包含一系列 "专家 "模块，而通用 LLM 则作为路由器，将查询路由到最合适的专家模块。这些模块可以是神经模块（如深度学习模型），也可以是符号模块（如数学计算器、货币转换器、天气 API）。</p>
<h1 id="hugginggpt">HuggingGPT<a aria-hidden="true" class="anchor-heading icon-link" href="#hugginggpt"></a></h1>
<p><img src="/assets/images/2023-09-21-22-49-51.png"></p>
<p>该系统包括 4 个阶段：</p>
<p>(1) 任务规划： LLM 充当大脑，将用户请求解析为多个任务。每个任务都有四个相关属性：任务类型、ID、依赖关系和参数。他们使用少量实例来指导 LLM 进行任务解析和规划。</p>
<p>指令：</p>
<blockquote>
<p>人工智能助手可以将用户输入解析为多个任务： {"task"：任务，"id"：task_id，"dep"：依赖关系_task_ids，"args"：参数： {"text": 文本，"image"： URL, "audio"： URL, "video"： URL}}]。dep "字段表示生成当前任务所依赖的新资源的前一个任务的 id。特殊标记"-task_id "指的是依赖任务中生成的文本图像、音频和视频，id 为 task_id。必须从以下选项中选择任务： {{可用任务列表 }}}。任务之间存在逻辑关系，请注意它们的顺序。如果无法解析用户输入，则需要回复空 JSON。以下是几个案例供您参考： {{ 演示 }}。聊天记录记录为 {{ 聊天记录 }}。从该聊天历史记录中，您可以找到用户提及的资源路径，以便进行任务规划。</p>
</blockquote>
<p>(2) 模型选择： LLM 会将任务分配给专家模型，其中的请求是一道多选题。LLM 会收到一份可供选择的模型列表。由于上下文长度有限，需要进行基于任务类型的过滤。</p>
<p>指令：</p>
<blockquote>
<p>给定用户请求和调用命令后，人工智能助手会帮助用户从模型列表中选择一个合适的模型来处理用户请求。人工智能助手仅输出最合适模型的模型 ID。输出必须采用严格的 JSON 格式： "id"： "id", "reason"： "您选择的详细原因"。我们有一个模型列表供您选择 {{ 候选模型 }}。请从列表中选择一个模型。</p>
</blockquote>
<p>(3) 任务执行： 专家模型执行特定任务并记录结果。</p>
<p>指示：</p>
<blockquote>
<p>有了输入和推理结果，人工智能助手需要描述过程和结果。前几个阶段可归纳为--用户输入： {{ 用户输入 }}、任务规划： {{ 任务 }}, 模型选择： {{ 模型分配 }}, 任务执行： {{ 预测 }}。首先，您必须直截了当地回答用户的请求。然后以第一人称向用户描述任务过程并展示您的分析和模型推理结果。如果推理结果包含文件路径，必须告诉用户完整的文件路径。</p>
</blockquote>
<p>(4) 生成响应： LLM 接收执行结果并向用户提供汇总结果。</p>
<p>要将 HuggingGPT 投入实际应用，需要解决几个难题：（1）需要提高效率，因为 LLM 推理轮和与其他模型的交互都会减慢进程；（2）它依赖于较长的上下文窗口来交流复杂的任务内容；（3）提高 LLM 输出和外部模型服务的稳定性。</p></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#mrkl" title="MRKL">MRKL</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#hugginggpt" title="HuggingGPT">HuggingGPT</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"82undztzy7arc9g53jgjoas","title":"tool","desc":"","updated":1695361847463,"created":1695361736583,"custom":{},"fname":"development.ml.LLM.tool","type":"note","vault":{"fsPath":"repos/dendron-aws-vault","remote":{"type":"git","url":"https://github.com/cczhong11/my_note.git"},"name":"my_note"},"contentHash":"7db8eae366e9b6985023540b022b0f6a","links":[],"anchors":{"mrkl":{"type":"header","text":"MRKL","value":"mrkl","line":8,"column":0,"depth":1},"hugginggpt":{"type":"header","text":"HuggingGPT","value":"hugginggpt","line":12,"column":0,"depth":1}},"children":[],"parent":"cki43m8kwb6iflb2de61itq","data":{}},"body":"\u003ch1 id=\"tool\"\u003etool\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#tool\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch1 id=\"mrkl\"\u003eMRKL\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#mrkl\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eMRKL（Karpas 等人，2022 年）是 \"模块化推理、知识和语言 \"的简称，是一种用于自主代理的神经符号架构。建议 MRKL 系统包含一系列 \"专家 \"模块，而通用 LLM 则作为路由器，将查询路由到最合适的专家模块。这些模块可以是神经模块（如深度学习模型），也可以是符号模块（如数学计算器、货币转换器、天气 API）。\u003c/p\u003e\n\u003ch1 id=\"hugginggpt\"\u003eHuggingGPT\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#hugginggpt\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e\u003cimg src=\"/assets/images/2023-09-21-22-49-51.png\"\u003e\u003c/p\u003e\n\u003cp\u003e该系统包括 4 个阶段：\u003c/p\u003e\n\u003cp\u003e(1) 任务规划： LLM 充当大脑，将用户请求解析为多个任务。每个任务都有四个相关属性：任务类型、ID、依赖关系和参数。他们使用少量实例来指导 LLM 进行任务解析和规划。\u003c/p\u003e\n\u003cp\u003e指令：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e人工智能助手可以将用户输入解析为多个任务： {\"task\"：任务，\"id\"：task_id，\"dep\"：依赖关系_task_ids，\"args\"：参数： {\"text\": 文本，\"image\"： URL, \"audio\"： URL, \"video\"： URL}}]。dep \"字段表示生成当前任务所依赖的新资源的前一个任务的 id。特殊标记\"-task_id \"指的是依赖任务中生成的文本图像、音频和视频，id 为 task_id。必须从以下选项中选择任务： {{可用任务列表 }}}。任务之间存在逻辑关系，请注意它们的顺序。如果无法解析用户输入，则需要回复空 JSON。以下是几个案例供您参考： {{ 演示 }}。聊天记录记录为 {{ 聊天记录 }}。从该聊天历史记录中，您可以找到用户提及的资源路径，以便进行任务规划。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e(2) 模型选择： LLM 会将任务分配给专家模型，其中的请求是一道多选题。LLM 会收到一份可供选择的模型列表。由于上下文长度有限，需要进行基于任务类型的过滤。\u003c/p\u003e\n\u003cp\u003e指令：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e给定用户请求和调用命令后，人工智能助手会帮助用户从模型列表中选择一个合适的模型来处理用户请求。人工智能助手仅输出最合适模型的模型 ID。输出必须采用严格的 JSON 格式： \"id\"： \"id\", \"reason\"： \"您选择的详细原因\"。我们有一个模型列表供您选择 {{ 候选模型 }}。请从列表中选择一个模型。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e(3) 任务执行： 专家模型执行特定任务并记录结果。\u003c/p\u003e\n\u003cp\u003e指示：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e有了输入和推理结果，人工智能助手需要描述过程和结果。前几个阶段可归纳为--用户输入： {{ 用户输入 }}、任务规划： {{ 任务 }}, 模型选择： {{ 模型分配 }}, 任务执行： {{ 预测 }}。首先，您必须直截了当地回答用户的请求。然后以第一人称向用户描述任务过程并展示您的分析和模型推理结果。如果推理结果包含文件路径，必须告诉用户完整的文件路径。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e(4) 生成响应： LLM 接收执行结果并向用户提供汇总结果。\u003c/p\u003e\n\u003cp\u003e要将 HuggingGPT 投入实际应用，需要解决几个难题：（1）需要提高效率，因为 LLM 推理轮和与其他模型的交互都会减慢进程；（2）它依赖于较长的上下文窗口来交流复杂的任务内容；（3）提高 LLM 输出和外部模型服务的稳定性。\u003c/p\u003e","noteIndex":{"id":"0f1b48c7-3a25-4016-83a5-15864d7803ad","title":"Development","vault":{"fsPath":"repos/dendron-aws-vault","remote":{"type":"git","url":"https://github.com/cczhong11/my_note.git"},"name":"my_note"},"type":"note","desc":"","links":[{"from":{"fname":"root","id":"c95dc4f5-23db-45e7-a15b-64582a183ccc","vaultName":"my_note"},"type":"backlink","position":{"start":{"line":8,"column":3,"offset":63},"end":{"line":8,"column":21,"offset":81},"indent":[]},"value":"development"}],"anchors":{},"fname":"development","updated":1618381238346,"created":1612940782409,"parent":null,"children":["400115e9-30f7-4a13-9776-db059bc9cd42","xsX5v3ZsyJ0i6gf9","1bc3b45a-b6f4-4150-87d3-5bd5b6eb8c24","6cea4852-6e7c-4140-b476-85c07b48a642","a40ef849-d301-4d74-a778-e6d9469dfb5d","09d9081f-3dff-453d-8488-7d2344cc8895","p11phg7nb10yw0wck1fyagq","92917ea3-452e-48dc-875e-5cd0002041db","2e151826-cb22-4d89-8ce0-71dad7204ce8","baa39444-0da9-4c55-8df7-2a6f8f787fa4","q4um3bc3st86ilkkvdjom6x","62daf50d-a39e-463f-aabd-be53790281fd","3524d0a7-be73-45d6-847e-c970f5c1c760","eac0f243-05b3-4b95-bec3-848e33edbc40","nc777xa48letazo00100hin"],"data":{},"custom":{"nav_order":0,"permalink":"/"},"body":"\n","contentHash":"38357962f5a50c6fd7d318dfa66bea90"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"previewV2Enabled":false,"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"dendronVersion":"0.90.0","vaults":[{"fsPath":"repos/dendron-aws-vault","remote":{"type":"git","url":"https://github.com/cczhong11/my_note.git"},"name":"my_note"},{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableUserTags":true,"enableHashTags":true,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableEditorDecorations":true,"enableHandlebarTemplates":true,"enableFullHierarchyNoteTitle":false,"enablePersistentHistory":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"automaticallyShowPreview":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["development","life"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"my_note","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","enableMermaid":true,"siteUrl":"https://notes.tczhong.com","siteFaviconPath":"favicon.ico","siteIndex":"development"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"82undztzy7arc9g53jgjoas"},"buildId":"2XI28xCTA7nnAuXRumXG6","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>
{"pageProps":{"note":{"id":"40bf8bae-6748-4638-a9ea-43213edc6fb7","title":"Zippydb","vault":{"fsPath":"repos/dendron-aws-vault","remote":{"type":"git","url":"https://github.com/cczhong11/my_note.git"},"name":"my_note"},"type":"note","desc":"","links":[],"anchors":{"zippydb":{"type":"header","value":"zippydb","line":8,"column":0},"arch":{"type":"header","value":"arch","line":14,"column":0},"write":{"type":"header","value":"write","line":22,"column":0},"read":{"type":"header","value":"read","line":26,"column":0},"primary-failover":{"type":"header","value":"primary-failover","line":30,"column":0},"a-failed-replica-rejoins":{"type":"header","value":"a-failed-replica-rejoins","line":47,"column":0},"sharding-and-load-balancing":{"type":"header","value":"sharding-and-load-balancing","line":53,"column":0},"asynchronous-replication-overview":{"type":"header","value":"asynchronous-replication-overview","line":60,"column":0}},"fname":"development.database.zippydb","updated":1617668655908,"created":1617667853401,"parent":"a40ef849-d301-4d74-a778-e6d9469dfb5d","children":[],"data":{},"custom":{},"contentHash":"8c9c99a1bef06f21d1986f95eda0f051"},"body":"<h1 id=\"zippydb\">Zippydb<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#zippydb\"></a></h1>\n<h1 id=\"zippydb-1\">zippydb<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#zippydb-1\"></a></h1>\n<p>fb internal distributed key-value databse</p>\n<p>single node based on rockdb</p>\n<h2 id=\"arch\">arch<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#arch\"></a></h2>\n<p>ZippyDB servers can be mapped to the roles in the Multi-Paxos protocol as follows:</p>\n<p>Primary server = proposer/leader + acceptor + learner\nSecondary server = acceptor + learner\nFollower server = learner (An ordinary ZippyDB user may safely skip the details of the mapping above, as it is only intended to map terminology for readers who are familiar with Paxos.)</p>\n<h2 id=\"write\">write<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#write\"></a></h2>\n<p><img src=\"/assets/images/2021-04-05-17-12-35.png\"></p>\n<h2 id=\"read\">read<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#read\"></a></h2>\n<p><img src=\"/assets/images/2021-04-05-17-13-08.png\"></p>\n<h2 id=\"primary-failover\">primary failover<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#primary-failover\"></a></h2>\n<p>When the primary fails, Zeus detects the failure through lost heartbeats and notifies ShardManager. ShardManager chooses a most appropriate secondary and sends it a message to convert it into a new primary. </p>\n<p>Specifically in this example, it is possible that, right before server X crashed, server X and server Y worked together to accept a write and server X has already sent the \"write-success\" response back to the client, but that write has not yet reached server Z. This is because server X and server Y form a majority and hence can accept a write without waiting for the \"accepted\" confirmation from server Z. Actually, server Z may never get the \"accept\" notification for this write from server X before server X crashed, e.g., due to slow network between server X and server Z. In this case, after server Z becomes the new primary, it needs to contact server Y to recover the missing write. The prepare phase of the Paxos protocol guarantees that the new primary will correctly discover all those missing writes, so long as only a minority of the replicas have failed.\nAfter bringing its local replica up-to-date, the new primary (server Z in this example) starts to handle reads and writes as normal. As shown in the figure below, the write path executes the following steps in sequence:</p>\n<ol>\n<li>When the client wants to send a new \"Put()\" request, the ServiceRouter library linked into the client notices that ShardManager has changed the shard's primary assignment, and automatically routes the request to the new primary (server Z).</li>\n<li>Server Z asks both server X and server Y to \"accept\" the write.</li>\n<li>Server Z gets the \"accepted\" confirmation from server Y, but gets no confirmation from server X.</li>\n<li>Since server Z collects a majority votes for the write (one from itself and another from server Y), server Z decides to commit the write and sends a \"write-success\" response back to the client. Server Z does not wait for server X to recover in order to accept the write. In other words, once server Z becomes the new primary, the failure of server X does not affect the database's availability .</li>\n<li>Server Z sends a \"commit\" message to server Y to inform server Y that an agreement has been reached on the write. This step is not shown in the figure for brevity.</li>\n</ol>\n<p><img src=\"/assets/images/2021-04-05-17-15-33.png\"></p>\n<p>The process of primary fail-over takes about 10 seconds, including the time for Zeus to detect lost heartbeats from the old primary</p>\n<h2 id=\"a-failed-replica-rejoins\">A Failed Replica Rejoins<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#a-failed-replica-rejoins\"></a></h2>\n<p>Server X synchronizes with the new primary (server Z) to bring its local database up-to-date. It then starts to process protocol messages as normal. See the example in the figure below.</p>\n<p><img src=\"/assets/images/2021-04-05-17-16-52.png\"></p>\n<h2 id=\"sharding-and-load-balancing\">Sharding and Load Balancing<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#sharding-and-load-balancing\"></a></h2>\n<p>In practice, each server runs a single ZippyDB process that can host multiple shards. In the figure below, each server hosts three shards: a primary for one shard, and two secondaries for two other shards. For example, server 1 hosts shard A's primary, shard D's secondary, and shard E's secondary. The three replicas of shard A are distributed across server 1 (primary), server 2 (secondary), and server 5 (secondary). ShardManager considers multiple factors in shard placement. In this figure, ShardManager places exactly one primary on each server for the purpose of load balancing, because a primary incurs a higher load than a secondary does. ShardManager may also place the different replicas of a shard across different clusters for the purpose of better fault tolerance.</p>\n<p><img src=\"/assets/images/2021-04-05-17-18-35.png\"></p>\n<h2 id=\"asynchronous-replication-overview\">Asynchronous Replication Overview<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#asynchronous-replication-overview\"></a></h2>\n<p><img src=\"/assets/images/2021-04-05-17-22-52.png\"></p>\n<p><img src=\"/assets/images/2021-04-05-17-23-23.png\"></p>","noteIndex":{"id":"0f1b48c7-3a25-4016-83a5-15864d7803ad","title":"Development","vault":{"fsPath":"repos/dendron-aws-vault","remote":{"type":"git","url":"https://github.com/cczhong11/my_note.git"},"name":"my_note"},"type":"note","desc":"","links":[{"from":{"fname":"root","id":"c95dc4f5-23db-45e7-a15b-64582a183ccc","vaultName":"my_note"},"type":"backlink","position":{"start":{"line":8,"column":3,"offset":63},"end":{"line":8,"column":21,"offset":81},"indent":[]},"value":"development"}],"anchors":{},"fname":"development","updated":1618381238346,"created":1612940782409,"parent":null,"children":["400115e9-30f7-4a13-9776-db059bc9cd42","xsX5v3ZsyJ0i6gf9","1bc3b45a-b6f4-4150-87d3-5bd5b6eb8c24","6cea4852-6e7c-4140-b476-85c07b48a642","a40ef849-d301-4d74-a778-e6d9469dfb5d","09d9081f-3dff-453d-8488-7d2344cc8895","p11phg7nb10yw0wck1fyagq","92917ea3-452e-48dc-875e-5cd0002041db","2e151826-cb22-4d89-8ce0-71dad7204ce8","baa39444-0da9-4c55-8df7-2a6f8f787fa4","q4um3bc3st86ilkkvdjom6x","62daf50d-a39e-463f-aabd-be53790281fd","3524d0a7-be73-45d6-847e-c970f5c1c760","eac0f243-05b3-4b95-bec3-848e33edbc40","nc777xa48letazo00100hin"],"data":{},"custom":{"nav_order":0,"permalink":"/"},"body":"\n","contentHash":"38357962f5a50c6fd7d318dfa66bea90"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"previewV2Enabled":false,"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"dendronVersion":"0.90.0","vaults":[{"fsPath":"repos/dendron-aws-vault","remote":{"type":"git","url":"https://github.com/cczhong11/my_note.git"},"name":"my_note"},{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"enableUserTags":true,"enableHashTags":true,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableEditorDecorations":true,"enableHandlebarTemplates":true,"enableFullHierarchyNoteTitle":false,"enablePersistentHistory":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"automaticallyShowPreview":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["development","life"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"my_note","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","enableMermaid":true,"siteUrl":"https://notes.tczhong.com","siteFaviconPath":"favicon.ico","siteIndex":"development"}}},"__N_SSG":true}